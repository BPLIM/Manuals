[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to Work with Pseudo-Data",
    "section": "",
    "text": "1 BPLIM Guide to working with pseudo-data\nThis document serves as a guide for researchers working with pseudo-data prepared by BPLIM. Pseudo-data here refers to data that is generated randomly but respects the metadata, the links, and the time structure of the original data. Its purpose is solely to help researchers prepare the Stata scripts for surrogate access (a.k.a. remote execution). Please read this article for a brief introduction to BPLIM’s workflow for working with confidential data for research."
  },
  {
    "objectID": "index.html#data-structure-and-links",
    "href": "index.html#data-structure-and-links",
    "title": "How to Work with Pseudo-Data",
    "section": "1.1 Data structure and links",
    "text": "1.1 Data structure and links\nTo prepare the pseudo-data BPLIM must work with the researchers to identify all original datasets to use in the project along with the linking variables. For each original dataset, BPLIM will create a metafile and a file with a sample of the id (linking) variables. These files are needed to create the pseudo-data. As mentioned in the article, BPLIM does not provide the actual pseudo-data, but instead provides a set of Stata do-files that will generate the pseudo-data."
  },
  {
    "objectID": "index.html#the-package",
    "href": "index.html#the-package",
    "title": "How to Work with Pseudo-Data",
    "section": "1.2 The package",
    "text": "1.2 The package\nBPLIM will prepare and send to the external researchers a compressed file with all the necessary files. For illustrative purposes, let’s assume that the name of the project is pxxx_BPLIM and that the researcher identified two original files to work with:“CB_2006” and “CB_2007”. The researcher will receive a zipped file, for example, pxxx_BPLIM_pseudo.zip. The researcher only has to unzip that file. This will create the following directory/file structure:\n.../package\n│\n├── ados/\n│\n├── dos/\n│   ├── generate_dummy_dos.do\n│   ├── master.do \n│   ├── CB_2006_dummy.do\n│   └── ...\n│   \n├── ids/\n│   ├── CB_2006_ID.dta\n│   ├── CB_2007_ID.dta\n│   └── ...\n│   \n├── metadata/\n│   ├── CB_2006_meta.xlsx\n│   ├── CB_2007_meta.xlsx\n│   └── ...\n│\n└── pxxx_BPLIM/\n    └── ...\nAll the files in the first four directories - ados, dos, ids, and metadata - are needed to generate the pseudo datasets. The directory pxxx_BPLIM is the project folder, where the researcher is supposed to develop his/her work. Below we will further detail the structure of the project folder."
  },
  {
    "objectID": "index.html#creating-the-pseudo-data",
    "href": "index.html#creating-the-pseudo-data",
    "title": "How to Work with Pseudo-Data",
    "section": "1.3 Creating the pseudo data",
    "text": "1.3 Creating the pseudo data\nAfter unzipping the file, the researcher should open the file master.do in Stata (located in the dos folder). Please make sure that the Stata working directory is …/package/dos (where the master.do file is located) because we use relative paths in this script to reference other necessary files. Running master.do will create the pseudo datasets and place them in the project directory pxxx_BPLIM under the folder initial_dataset. The project directory has the following structure:\n.../package/pxxx_BPLIM/\n│\n├── initial_dataset/\n│   ├── CB_D_2006.dta\n│   ├── CB_D_2007.dta\n│   └── ...\n│\n├── results/\n│   └── ...\n│   \n├── tools/\n│   └── ...\n│\n└── work_area\n    ├── profile.do\n    └── template.do\nwhere we show the pseudo-data files that were created. These files will contain “_D_” in their names to reflect the fact that they are “dummy” data. After creating the pseudo data, you may relocate the project directory wherever you wish. However, the structure of the project directory must remain the same, because it mirrors the structure that BPLIM (or an internal user) has to replicate the code using original data. You should not copy any additional data files to the folder initial_dataset."
  },
  {
    "objectID": "index.html#setting-up-the-project",
    "href": "index.html#setting-up-the-project",
    "title": "How to Work with Pseudo-Data",
    "section": "1.4 Setting up the project",
    "text": "1.4 Setting up the project\n\n1.4.1 Modifying the profile.do\nAfter creating the pseudo data, the researcher is ready to start coding. The researcher should place all scripts in the work_area directory. In this area, the researcher is free to organize the code as it pleases. However, you may have noticed that this folder already contains two files: “template.do” and “profile.do”. As the name suggests, template.do is a template that you should use to prepare your scripts. But before you can start coding you must edit profile.do. This do-file sets all the configurations (globals, paths, etc.) and must be placed in the same directory as the script file that you are executing. Stata will run this file first automatically. For example, suppose that you decide to place your project directory under the folder C:/Users/Jane/. In that case you edit profile.do and adjust the global root_path, to “C:/Users/Jane/pxxx_BPLIM”, the path to the project directory.\n*********************************************************\n*            Initialization                              \n*********************************************************\nversion 18\nclear all\nprogram drop _all\nset more off\nset rmsg on\nset matsize 10000\nset linesize 255\ncapture log close\n*********************************************************\n*               Define globals                          *\n********************************************************* \n**** Path for replication ****\n* Root path\nglobal root_path \"C:/Users/Jane/pxxx_BPLIM\"    /* changed here  */\n* Base path for replications\nglobal path_rep \"${root_path}/work_area\"\n\n**** Paths for data ****\n* Set the path for non perturbed data source\nglobal path_source \"${root_path}/initial_dataset\"\n\n**** Globals for type of modified dataset\n* Dummy \nglobal M1 \"D\"\n/*********************************************************************\nExample: pseudo data \n\nuse \"${path_source}/SLB_${M1}_YBNK_20102018_OCT20_QA1_V01.dta\"\n*********************************************************************/\n\n**** Path for project specific ado files ****\nadopath ++ \"${root_path}/tools\"\n\n* Change ados path\nsysdir set PLUS \"${root_path}/tools\"\nlocal places \"SITE PERSONAL OLDPLACE\"\nforeach p of local places {\n  capture adopath - `p'\n}\nNote that the globals path_rep and path_source are built based on root_path. Global M1 is used to reference the type of dataset being used. This means that when writing your scripts you must always use the global M1 when referring to the datasets. For example, when reading the data do not write:\nuse \"${path_source}/CB_D_2006.dta\"\nbut instead use the global in the name of the file:\nuse \"${path_source}/CB_${M1}_2006.dta\"\n\n\n1.4.2 Installing tools for your project\nAll the external user-written ados must be installed in the tools directory of your project. We recommend using adoinstall for this purpose. Fo example, to install reghdfe in the tools directory, you need to type the following in Stata:\nssc install adoinstall\nadoinstall reghdfe, to(\"C:/Users/Jane/package/pxxx_BPLIM/tools\")\nThe scripts that you prepare should only use user-written commands that are placed in the tools directory of your project."
  },
  {
    "objectID": "index.html#preparing-your-code",
    "href": "index.html#preparing-your-code",
    "title": "How to Work with Pseudo-Data",
    "section": "1.5 Preparing your code",
    "text": "1.5 Preparing your code\nIt is really important to follow the above guidelines to ensure that results can be safely replicated on the original data. If the code is well organized and follows the guidelines then BPLIM staff (or an internal co-author) can easily change the configuration file profile.do to rerun the analysis on the original data. The template file that we provide shows examples on how the researcher should code, based on the settings defined in profile.do. Below we show the contents of template.do:\n* Project      :  pxxx_BPLIM\n* Author(s)    :\n* Date         :\n* Description  :\n* Dependencies :\n* Modifications: (add date, author and change)\n\n* Run profile (usually not needed, but just to be sure)\ncapture run \"profile.do\"\n\n* Change to work path - global `path_rep` defined in profile.do\ncd \"${path_rep}\"\n\n/* You may create a `results` folder inside `path_rep` to save outputs (this \nis optional since there is already a `results` folder outside `path_rep`)\nAlways use capture when creating directories in scripts*/\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n\n/*\nWhen defining globals for paths (if you do not want to use relative paths), remember to\ninclude the global `path_rep`. This is the path where the analysis should run. See the\ntwo examples below, where we define two globals for separate results folders\n*/\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\n\n* Creating a log file in the work area, where \"logexample\" is the log requested for extraction\nlog using \"logexample.log\", replace\n\n*********************************************************\n*                  Open data files                      *\n*********************************************************\n/*\nPlease note the VERY IMPORTANT use of global `M1`, ${M1}, \nin the file names of the modified data. Failing to use the \nglobals when working with modified data will cause the \nREPLICATION TO FAIL.\n*/\n\n* Example on how to read a dummy data file provided by BPLIM:\nuse \"${path_source}/CB_${M1}_2006\", clear\n\n*********************************************************\n*            Start data analysis                        *\n*********************************************************\n*\n* ...\n* YOUR STATA CODE GOES HERE\n* ...\n* You may call other do-files, just be sure to use the\n* globals defined in the profile and eventually in this\n* file\n\n*********************************************************\n*            Saving the results                             *\n*********************************************************\n\n* Saving an intermediate dataset. You may save it in your\n* work area, if you wish to preserve it in order to analyze\n* it later. However, in a replication, if these datasets are\n* not needed, you can delete them. As an example, see below:\n\n* Create intermediate data directory\ncap mkdir intermediate_data\n\n* Add global for directory\nglobal path_data \"${path_rep}/intermediate_data\"\n\n* Save datasets\ncompress\nsave \"${path_data}/filename1.dta\", replace\nsave \"${path_data}/filename2.dta\", replace\n\n* Remove intermediate data after analysis is finished\nlocal files: dir \"${path_data}\" files \"*.dta\", respectcase\nforeach file of local files {\n  rm \"${path_data}/`file'\"\n}\n\n****** Results Examples ******\n* Creating a graph and saving to the results area (figures)\ngraph export \"${results_figures}/mygraph.png\", replace\n* Creating a table and saving to the results area (tables)\nesttab using \"${results_tables}/myfile.tex\", cells(\"cell1 cell2 ...\")\n\n*********************************************************\n*                  Close the log file                   *\n*********************************************************\nlog close\n\n***************** IMPORTANT *****************\n\n* BPLIM reserves the right to decline to send log files that are \n* exceptionally large. For really long scripts, remember that not\n* every line of code and its output needs to be in the log file.\n* Take the following example:\n/*\nlog using \"mylog.log\", replace\n\nquietly {\n  sysuse auto, clear\n  summarize price\n  noisily display \"Mean Price: `r(mean)'\"\n  drop if foreign == 1\n  keep price mpg rep78\n  noisily reg price mpg i.rep78\n}\n\nlog close\n*/\n\n* Only the outputs of the third and sixth lines will appear in the\n* log file. This is a good strategy to follow if your log files are \n* too cluttered with code and output, which are only intermediate steps\n* to final outputs\nNotice the first lines of the template, where we run the configuration file - capture run \"profile.do\" - and change directory to the work area folder - cd \"${path_rep}\". Note that global path_rep is defined in the configuration file. We also create new folders in our working directory and set globals in order to reference them later:\ncapture mkdir results\n* You may create the structure that you want, adding sub-diectories to `results`\ncapture mkdir results/tables\ncapture mkdir results/figures\n...\nglobal results_tables \"${path_rep}/results/tables\"\nglobal results_figures \"${path_rep}/results/figures\"\nThe remaining lines of the template contain standard examples of how to generate outputs and organize your code.\n\n1.5.1 Create a master script\nResearchers should create a master script that runs their analysis from top to bottom. This file should be based on template.do and must create a log file that proves that the code ran without errors. Still, researchers are free to organize code in the work_area as they please. For example, the following structure would be acceptable:\n.../package/pxxx_BPLIM/work_area/\n│\n├── profile.do\n├── master.do\n│\n├── 01_data_management/\n│   ├── 01_data_manipulation.do\n│   └── ...\n│\n├── 02_exploratory/\n│   ├── 01_tables.do\n│   ├── 02_figures.do\n│   └── ...\n│\n└── 03_regressions/\n    ├── 01_regressions.do\n    └── ...\nThen, within master.do, the researcher only has to run the dependencies:\n...\ncd ${path_rep}\n...\n\ndo 01_data_management/01_data_manipulation.do\n\ndo 02_exploratory/01_tables.do\ndo 02_exploratory/02_figures.do\n\ndo 03_regressions/01_regressions.do\n\nlog close"
  },
  {
    "objectID": "index.html#creating-the-replicability-package",
    "href": "index.html#creating-the-replicability-package",
    "title": "How to Work with Pseudo-Data",
    "section": "1.6 Creating the replicability package",
    "text": "1.6 Creating the replicability package\nAfter completing your analysis, you must prepare your replication package to send to BPLIM. With that package, BPLIM wil be able to replicate your results using the original data.\nPreparing the package is a simple procedure. All you need to do is run ado archive_rep, which will zip all the necessary files for the replication. It also generates a list of all the files used and creates a requirements file with all the dependencies (ados). Please note that only script files (ados, dos, etc) are copied to the archive. In this case, we would run the following in Stata:\nadopath + \"C:/Users/Jane/pxxx_BPLIM/tools\"\narchive_rep, rep(1) path(\"C:/Users/Jane/pxxx_BPLIM\")\nThe output of the command is a folder named Rep001 with the folders and files (scripts) needed for the replication. This folder, and the zip file Rep001.zip (zip file of the folder) are created in the work_area directory. The researcher only has to send the zip file to BPLIM."
  },
  {
    "objectID": "index.html#recap",
    "href": "index.html#recap",
    "title": "How to Work with Pseudo-Data",
    "section": "1.7 Recap",
    "text": "1.7 Recap\nIt is important that the researcher follows the guidelines and the proper workflow:\n\nResearcher identifies the datasets and tools for the project\nBPLIM staff prepares the package and sends it to the researcher in a zip file\nResearcher unpacks the file and creates the project structure\nResearcher runs the do-file to create the pseudo data in the project initial dataset folder\nResearcher adapts profile.do, namely the global root_path, so that it points to the correct directory in her computer\nResearcher creates a master.do file based on template.do (it can be copied) and organizes the code as she desires\nAfter the analysis is finished, the researcher runs the ado archive_rep and sends the output zip file to BPLIM. Researchers should clearly indicate the output files needed for the analysis.\nBPLIM checks that the analysis runs from top to bottom. In case that it does, the staff will run the analysis on the original data\nBPLIM checks the output files requested by the researchers and emails the results if they respect the output control rules.\n\nImportant note:\n\nIntermediate datasets must be created during the analysis. Remember that the researcher is only allowed to send scripts and logs for replications, so BPLIM only has access to datasets in the initial_dataset folder. Trying to reference intermediate data that is not created during the analysis will cause the replication to fail."
  }
]